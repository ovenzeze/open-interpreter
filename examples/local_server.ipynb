{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a local Open Interpreter server for a custom front end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Python Environment:\n",
      "   Python Executable: /Users/clay/Library/Caches/pypoetry/virtualenvs/open-interpreter-2ItoNx9d-py3.9/bin/python\n",
      "\n",
      "2. Directory Information:\n",
      "   Current Directory: /Users/clay/Code/open-interpreter/examples\n",
      "   Project Root: /Users/clay/Code/open-interpreter\n",
      "\n",
      "3. Project Root already in Python Path\n",
      "\n",
      "4. Interpreter Import:\n",
      "   Path: /Users/clay/Code/open-interpreter/interpreter/__init__.py\n",
      "   Using Development Version: True\n",
      "\n",
      "5. Python Path (first 5 entries):\n",
      "   - /Users/clay/.pyenv/versions/3.9.5/lib/python39.zip\n",
      "   - /Users/clay/.pyenv/versions/3.9.5/lib/python3.9\n",
      "   - /Users/clay/.pyenv/versions/3.9.5/lib/python3.9/lib-dynload\n",
      "   - \n",
      "   - /Users/clay/Library/Caches/pypoetry/virtualenvs/open-interpreter-2ItoNx9d-py3.9/lib/python3.9/site-packages\n",
      "\n",
      "Environment Verification: ✅ PASSED\n"
     ]
    }
   ],
   "source": [
    "# Development Environment Verification\n",
    "# This cell helps verify that we're using the local development version of the interpreter\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def verify_dev_environment():\n",
    "    \"\"\"\n",
    "    Verify the development environment setup for open-interpreter\n",
    "    Returns:\n",
    "        bool: True if everything is correctly set up\n",
    "    \"\"\"\n",
    "    # 1. Check Python Interpreter and Environment\n",
    "    print(\"1. Python Environment:\")\n",
    "    print(f\"   Python Executable: {sys.executable}\")\n",
    "    \n",
    "    # 2. Check Working Directory\n",
    "    current_dir = os.getcwd()\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "    print(\"\\n2. Directory Information:\")\n",
    "    print(f\"   Current Directory: {current_dir}\")\n",
    "    print(f\"   Project Root: {project_root}\")\n",
    "    \n",
    "    # 3. Ensure Project Root is in Python Path\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "        print(\"\\n3. Added Project Root to Python Path\")\n",
    "    else:\n",
    "        print(\"\\n3. Project Root already in Python Path\")\n",
    "    \n",
    "    # 4. Import and Verify Interpreter\n",
    "    try:\n",
    "        import interpreter\n",
    "        interpreter_path = interpreter.__file__\n",
    "        print(\"\\n4. Interpreter Import:\")\n",
    "        print(f\"   Path: {interpreter_path}\")\n",
    "        \n",
    "        # Verify if using local development version\n",
    "        is_dev_version = project_root in interpreter_path\n",
    "        print(f\"   Using Development Version: {is_dev_version}\")\n",
    "        \n",
    "        if not is_dev_version:\n",
    "            print(\"   WARNING: Not using the local development version!\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"\\n4. ERROR: Failed to import interpreter!\")\n",
    "        return False\n",
    "    \n",
    "    # 5. Display first few Python path entries\n",
    "    print(\"\\n5. Python Path (first 5 entries):\")\n",
    "    for path in sys.path[:5]:\n",
    "        print(f\"   - {path}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run verification\n",
    "is_dev_env_ready = verify_dev_environment()\n",
    "print(\"\\nEnvironment Verification:\", \"✅ PASSED\" if is_dev_env_ready else \"❌ FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install flask python-dotenv requests boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 00:09:32 [INFO] Current model: bedrock/anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      "Open Interpreter server is running on http://0.0.0.0:6000\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 00:09:32 [INFO] \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:6000\n",
      " * Running on http://192.168.86.246:6000\n",
      "2025-02-15 00:09:32 [INFO] \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2025-02-15 00:10:25 [INFO] 127.0.0.1 - - [15/Feb/2025 00:10:25] \"\u001b[33mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" 404 -\n",
      "2025-02-15 00:10:41 [INFO] 127.0.0.1 - - [15/Feb/2025 00:10:41] \"\u001b[33mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" 404 -\n",
      "2025-02-15 00:12:16 [INFO] Request received - IP: 127.0.0.1, Prompt: 你好，请帮我检查当前设备的系统信息。\n",
      "\u001b[92m00:12:16 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= anthropic.claude-3-sonnet-20240229-v1:0; provider = bedrock\n",
      "2025-02-15 00:12:16 [INFO] \n",
      "LiteLLM completion() model= anthropic.claude-3-sonnet-20240229-v1:0; provider = bedrock\n",
      "2025-02-15 00:12:17 [INFO] Found credentials in environment variables.\n",
      "2025-02-15 00:12:17 [INFO] HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/converse-stream \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m00:12:19 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= anthropic.claude-3-sonnet-20240229-v1:0; provider = bedrock\n",
      "2025-02-15 00:12:19 [INFO] \n",
      "LiteLLM completion() model= anthropic.claude-3-sonnet-20240229-v1:0; provider = bedrock\n",
      "2025-02-15 00:12:19 [INFO] Found credentials in environment variables.\n",
      "/Users/clay/Library/Caches/pypoetry/virtualenvs/open-interpreter-2ItoNx9d-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-15 00:12:20 [INFO] HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/converse-stream \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m00:12:23 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= anthropic.claude-3-sonnet-20240229-v1:0; provider = bedrock\n",
      "2025-02-15 00:12:23 [INFO] \n",
      "LiteLLM completion() model= anthropic.claude-3-sonnet-20240229-v1:0; provider = bedrock\n",
      "2025-02-15 00:12:23 [INFO] Found credentials in environment variables.\n",
      "2025-02-15 00:12:23 [INFO] HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/converse-stream \"HTTP/1.1 200 OK\"\n",
      "2025-02-15 00:12:25 [INFO] Request processed successfully - Prompt: 你好，请帮我检查当前设备的系统信息。...\n",
      "2025-02-15 00:12:25 [INFO] 127.0.0.1 - - [15/Feb/2025 00:12:25] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Flask Server Setup for Open Interpreter\n",
    "# This server provides a REST API interface for the Open Interpreter\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from interpreter import interpreter\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from threading import Thread\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check required environment variables\n",
    "required_env_vars = ['LITELLM_MODEL']  # Add other required env vars as needed\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    logger.error(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "    logger.info(\"Please ensure the following variables are set in your .env file:\")\n",
    "    for var in missing_vars:\n",
    "        logger.info(f\"  {var}=your-key-here\")\n",
    "\n",
    "# Interpreter Configuration\n",
    "interpreter.llm.model = os.environ.get('LITELLM_MODEL', 'gpt-3.5-turbo')\n",
    "interpreter.llm.context_window = 10000\n",
    "interpreter.llm.max_tokens = 4096\n",
    "interpreter.auto_run = True\n",
    "interpreter.verbose = True\n",
    "\n",
    "# Chat endpoint\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    \"\"\"\n",
    "    Handle chat requests\n",
    "    Expected payload: {\"prompt\": \"User's message or question\"}\n",
    "    Returns: {\n",
    "        \"response\": \"AI's response\",\n",
    "        \"code\": \"Generated code (if any)\",\n",
    "        \"output\": \"Code execution output (if any)\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    prompt = data.get('prompt')\n",
    "    \n",
    "    logger.info(f\"Request received - IP: {request.remote_addr}, Prompt: {prompt}\")\n",
    "    \n",
    "    if not prompt:\n",
    "        logger.warning(\"Empty prompt received\")\n",
    "        return jsonify({\"error\": \"No prompt provided\"}), 400\n",
    "\n",
    "    if missing_vars:\n",
    "        error_msg = f\"Server configuration error: Missing required variables {', '.join(missing_vars)}\"\n",
    "        logger.error(error_msg)\n",
    "        return jsonify({\"error\": error_msg}), 500\n",
    "\n",
    "    full_response = \"\"\n",
    "    current_code_block = []\n",
    "    code_blocks = []\n",
    "    execution_outputs = []\n",
    "    \n",
    "    try:\n",
    "        for chunk in interpreter.chat(prompt, stream=True, display=False):\n",
    "            if isinstance(chunk, dict):\n",
    "                chunk_type = chunk.get(\"type\")\n",
    "                content = chunk.get(\"content\", \"\")\n",
    "                \n",
    "                if chunk_type == \"message\":\n",
    "                    # 如果之前有未完成的代码块，先保存它\n",
    "                    if current_code_block:\n",
    "                        code_blocks.append(\"\".join(current_code_block))\n",
    "                        current_code_block = []\n",
    "                    full_response += content\n",
    "                elif chunk_type == \"code\":\n",
    "                    # 收集代码块片段\n",
    "                    current_code_block.append(content)\n",
    "                elif chunk_type == \"output\":\n",
    "                    execution_outputs.append(content)\n",
    "            \n",
    "            elif isinstance(chunk, str):\n",
    "                try:\n",
    "                    json_chunk = json.loads(chunk)\n",
    "                    full_response += json_chunk.get(\"response\", \"\")\n",
    "                except json.JSONDecodeError:\n",
    "                    full_response += chunk\n",
    "        \n",
    "        # 确保最后的代码块也被保存\n",
    "        if current_code_block:\n",
    "            code_blocks.append(\"\".join(current_code_block))\n",
    "        \n",
    "        # 清理代码块，移除多余的空行和格式问题\n",
    "        cleaned_code_blocks = []\n",
    "        for code in code_blocks:\n",
    "            # 移除多余的空行\n",
    "            lines = [line for line in code.split('\\n') if line.strip()]\n",
    "            # 重新组合代码，确保格式正确\n",
    "            cleaned_code = '\\n'.join(lines)\n",
    "            cleaned_code_blocks.append(cleaned_code)\n",
    "        \n",
    "        logger.info(f\"Request processed successfully - Prompt: {prompt[:50]}...\")\n",
    "        return jsonify({\n",
    "            \"response\": full_response.strip(),\n",
    "            \"code\": \"\\n\\n\".join(cleaned_code_blocks) if cleaned_code_blocks else None,\n",
    "            \"output\": \"\\n\".join(execution_outputs) if execution_outputs else None\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"Error processing request - Prompt: {prompt}, Error: {error_msg}\")\n",
    "        return jsonify({\"error\": f\"Error processing request: {error_msg}\"}), 500\n",
    "# Shutdown endpoint\n",
    "@app.route('/shutdown', methods=['GET'])\n",
    "def shutdown():\n",
    "    \"\"\"Shutdown the Flask server gracefully\"\"\"\n",
    "    func = request.environ.get('werkzeug.server.shutdown')\n",
    "    if func is None:\n",
    "        raise RuntimeError('Not running with the Werkzeug Server')\n",
    "    func()\n",
    "    return 'Server shutting down...'\n",
    "\n",
    "# Server control functions\n",
    "server_thread = None\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start the Flask server in a background thread\"\"\"\n",
    "    global server_thread\n",
    "    if server_thread and server_thread.is_alive():\n",
    "        print(\"Server is already running\")\n",
    "        return\n",
    "    \n",
    "    def run_flask_app():\n",
    "        app.run(host='0.0.0.0', port=6000)\n",
    "    \n",
    "    server_thread = Thread(target=run_flask_app)\n",
    "    server_thread.daemon = True\n",
    "    server_thread.start()\n",
    "    logger.info(f\"Current model: {interpreter.llm.model}\")\n",
    "    print(\"Open Interpreter server is running on http://0.0.0.0:6000\")\n",
    "\n",
    "def stop_server():\n",
    "    \"\"\"Stop the Flask server gracefully\"\"\"\n",
    "    import requests\n",
    "    try:\n",
    "        requests.get('http://localhost:6000/shutdown')\n",
    "        print(\"Server stopped successfully\")\n",
    "    except:\n",
    "        print(\"Server already stopped or not running\")\n",
    "\n",
    "# Start the server in background thread\n",
    "start_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_interpreter(prompt):\n",
    "    \"\"\"\n",
    "    Test the Open Interpreter server with formatted output\n",
    "    Args:\n",
    "        prompt: The prompt to send to the server\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            'http://localhost:6000/chat',\n",
    "            json={\"prompt\": prompt},\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            \n",
    "            # Display the response in a structured way using Markdown\n",
    "            md_output = \"## AI Response\\n\"\n",
    "            md_output += result.get(\"response\", \"No response\") + \"\\n\"\n",
    "            \n",
    "            if result.get(\"code\"):\n",
    "                md_output += \"\\n## Generated Code\\n```python\\n\"\n",
    "                # 确保代码块格式正确\n",
    "                code = result.get(\"code\").strip()\n",
    "                md_output += code + \"\\n```\\n\"\n",
    "            \n",
    "            if result.get(\"output\"):\n",
    "                md_output += \"\\n## Execution Output\\n```\\n\"\n",
    "                # 清理输出格式\n",
    "                output = result.get(\"output\").strip()\n",
    "                md_output += output + \"\\n```\\n\"\n",
    "            \n",
    "            display(Markdown(md_output))\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.json().get('error', 'Unknown error occurred'))\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Error: Could not connect to server. Make sure the server is running on port 5001.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Server control with proper shutdown mechanism\n",
    "import requests\n",
    "import signal\n",
    "import os\n",
    "\n",
    "def test_shutdown():\n",
    "    \"\"\"\n",
    "    Test the server shutdown endpoint with process termination\n",
    "    Returns:\n",
    "        bool: True if shutdown was successful\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try to find the process using port 5001\n",
    "        import psutil\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'connections']):\n",
    "            try:\n",
    "                for conn in proc.connections():\n",
    "                    if conn.laddr.port == 5001:\n",
    "                        print(f\"Found server process: {proc.pid}\")\n",
    "                        # Send SIGTERM signal to the process\n",
    "                        os.kill(proc.pid, signal.SIGTERM)\n",
    "                        print(\"Server shutdown signal sent successfully\")\n",
    "                        return True\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "                continue\n",
    "                \n",
    "        print(\"No server process found on port 5001\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during shutdown: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Execute shutdown test\n",
    "test_shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-interpreter-2ItoNx9d-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
