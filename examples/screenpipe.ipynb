{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Interpreter and ScreenPipe Cookbook\n",
    "\n",
    "This cookbook explores the powerful combination of Open Interpreter and ScreenPipe, two tools that can significantly enhance your ability to interact with and process digital information. Open Interpreter allows you to execute natural language commands, while ScreenPipe captures and analyzes screen content and audio output from your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have installed the following:\n",
    "\n",
    "1. [Open Interpreter](https://docs.openinterpreter.com/getting-started/introduction)\n",
    "2. [Screenpipe CLI](https://docs.screenpi.pe/docs/getting-started#cli-installation)\n",
    "\n",
    "Make sure both are properly installed and configured on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Open Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time: 2025-02-10 23:13:07 UTC\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from interpreter import interpreter\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Configure Open Interpreter\n",
    "interpreter.llm.model = \"groq/llama-3.1-70b-versatile\"\n",
    "interpreter.computer.import_computer_api = False\n",
    "interpreter.llm.supports_functions = False\n",
    "interpreter.llm.supports_vision = False\n",
    "interpreter.llm.context_window = 100000\n",
    "interpreter.llm.max_tokens = 4096\n",
    "\n",
    "# Add the current date and time in UTC\n",
    "current_datetime = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "print(f\"Current date and time: {current_datetime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the ScreenPipe Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScreenPipe search function defined and added to the interpreter's environment.\n"
     ]
    }
   ],
   "source": [
    "# Define the custom ScreenPipe search function\n",
    "custom_tool = \"\"\"\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "\n",
    "def search_screenpipe(query, limit=5, start_time=None, end_time=None):\n",
    "    base_url = f\"http://localhost:3030/search?q={quote(query)}&content_type=ocr&limit={limit}\"\n",
    "    \n",
    "    if start_time:\n",
    "        base_url += f\"&start_time={quote(start_time)}\"\n",
    "    if end_time:\n",
    "        base_url += f\"&end_time={quote(end_time)}\"\n",
    "    \n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Remove duplicates based on text content\n",
    "        unique_results = []\n",
    "        seen_texts = set()\n",
    "        for item in data[\"data\"]:\n",
    "            text = item[\"content\"][\"text\"]\n",
    "            if text not in seen_texts:\n",
    "                unique_results.append(item)\n",
    "                seen_texts.add(text)\n",
    "        return unique_results\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch data from ScreenPipe. Status code: {response.status_code}\"\n",
    "\"\"\"\n",
    "\n",
    "# Add the custom tool to the interpreter's environment\n",
    "interpreter.computer.run(\"python\", custom_tool)\n",
    "print(\"ScreenPipe search function defined and added to the interpreter's environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Custom Instructions for Open Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom instructions set for Open Interpreter.\n"
     ]
    }
   ],
   "source": [
    "# Set custom instructions for Open Interpreter\n",
    "interpreter.custom_instructions = f\"\"\"\n",
    "Current date and time: {current_datetime}\n",
    "\n",
    "ScreenPipe is a powerful tool that continuously captures and indexes the content displayed on your screen. It creates a searchable history of everything you've seen or interacted with on your computer. This includes text from websites, documents, applications, and even images (through OCR).\n",
    "\n",
    "You have access to this wealth of information through the `search_screenpipe(query, limit=5, start_time=None, end_time=None)` function. This allows you to provide more contextual and personalized assistance based on the user's recent activities and viewed content.\n",
    "\n",
    "The `search_screenpipe` function supports optional `start_time` and `end_time` parameters to narrow down the search to a specific time range. The time format should be ISO 8601, like this: \"2024-10-16T12:00:00Z\".\n",
    "\n",
    "Here's why querying ScreenPipe is valuable:\n",
    "1. Context Recall: Users often refer to things they've seen recently but may not remember the exact details. ScreenPipe can help recall this information.\n",
    "2. Information Verification: You can cross-reference user claims or questions with actual content they've viewed.\n",
    "3. Personalized Assistance: By knowing what the user has been working on or researching, you can provide more relevant advice and suggestions.\n",
    "4. Productivity Enhancement: You can help users quickly locate information they've seen before but can't remember where.\n",
    "\n",
    "Use the `search_screenpipe()` function when:\n",
    "- The user asks about something they've seen or read recently.\n",
    "- You need to verify or expand on information the user mentions.\n",
    "- You want to provide context-aware suggestions or assistance.\n",
    "- The user is trying to recall specific details from their recent computer usage.\n",
    "- The user wants to search within a specific time range.\n",
    "\n",
    "Here's how to use it effectively:\n",
    "1. When a user's query relates to recent activities or viewed content, identify key terms for the search.\n",
    "2. If the user specifies a time range, use the `start_time` and `end_time` parameters.\n",
    "3. Call the `search_screenpipe()` function with these parameters.\n",
    "4. Analyze the results to find relevant information.\n",
    "5. Summarize the findings for the user, mentioning the source (app name, window name) and when it was seen (timestamp).\n",
    "\n",
    "Remember to use this tool proactively when you think it might help answer the user's question, even if they don't explicitly mention ScreenPipe.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Custom instructions set for Open Interpreter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Open Interpreter</span> will require approval before running code.                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mOpen Interpreter\u001b[0m will require approval before running code.                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Use <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">interpreter -y</span> to bypass this.                                                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Use \u001b[1;36;40minterpreter -y\u001b[0m to bypass this.                                                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Press <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">CTRL-C</span> to exit.                                                                                              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Press \u001b[1;36;40mCTRL-C\u001b[0m to exit.                                                                                              \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 1582, in completion\n    response = groq_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/groq/chat/handler.py\", line 55, in completion\n    return super().completion(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/openai_like/chat/handler.py\", line 248, in completion\n    api_base, headers = self._validate_environment(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/openai_like/common_utils.py\", line 30, in _validate_environment\n    raise OpenAILikeError(\nlitellm.llms.openai_like.common_utils.OpenAILikeError: Missing API Key - A call is being made to LLM Provider but no key is set either in the environment variables ({LLM_PROVIDER}_API_KEY) or via params\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/respond.py\", line 87, in respond\n    for chunk in interpreter.llm.run(messages_for_llm):\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 324, in run\n    yield from run_text_llm(self, params)\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/llm/run_text_llm.py\", line 20, in run_text_llm\n    for chunk in llm.completions(**params):\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 466, in fixed_litellm_completions\n    raise first_error  # If all attempts fail, raise the first error\n    ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 443, in fixed_litellm_completions\n    yield from litellm.completion(**params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 1156, in wrapper\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 1034, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 3085, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2201, in exception_type\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 381, in exception_type\n    raise BadRequestError(\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: GroqException - Missing API Key - A call is being made to LLM Provider but no key is set either in the environment variables ({LLM_PROVIDER}_API_KEY) or via params\n\n\nThere might be an issue with your API key(s).\n\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here'. Update your ~/.zshrc on MacOS or ~/.bashrc on Linux with the new key if it has already been persisted there.,\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAILikeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/main.py:1582\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m             optional_params[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m-> 1582\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgroq_chat_completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maiohttp_openai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;66;03m# NEW aiohttp provider for 10-100x higher RPS\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/llms/groq/chat/handler.py:55\u001b[0m, in \u001b[0;36mGroqChatCompletion.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, acompletion, litellm_params, logger_fn, headers, timeout, client, custom_endpoint, streaming_decoder, fake_stream)\u001b[0m\n\u001b[1;32m     53\u001b[0m     fake_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreaming_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreaming_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfake_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai_like/chat/handler.py:248\u001b[0m, in \u001b[0;36mOpenAILikeChatHandler.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, acompletion, litellm_params, logger_fn, headers, timeout, client, custom_endpoint, streaming_decoder, fake_stream)\u001b[0m\n\u001b[1;32m    247\u001b[0m base_model: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m optional_params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 248\u001b[0m api_base, headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m optional_params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai_like/common_utils.py:30\u001b[0m, in \u001b[0;36mOpenAILikeBase._validate_environment\u001b[0;34m(self, api_key, api_base, endpoint_type, headers, custom_endpoint)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m headers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAILikeError(\n\u001b[1;32m     31\u001b[0m         status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m     32\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing API Key - A call is being made to LLM Provider but no key is set either in the environment variables (\u001b[39m\u001b[38;5;132;01m{LLM_PROVIDER}\u001b[39;00m\u001b[38;5;124m_API_KEY) or via params\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAILikeError\u001b[0m: Missing API Key - A call is being made to LLM Provider but no key is set either in the environment variables ({LLM_PROVIDER}_API_KEY) or via params",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/respond.py:87\u001b[0m, in \u001b[0;36mrespond\u001b[0;34m(interpreter)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_for_llm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py:324\u001b[0m, in \u001b[0;36mLlm.run\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m run_text_llm(\u001b[38;5;28mself\u001b[39m, params)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/llm/run_text_llm.py:20\u001b[0m, in \u001b[0;36mrun_text_llm\u001b[0;34m(llm, params)\u001b[0m\n\u001b[1;32m     18\u001b[0m language \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py:466\u001b[0m, in \u001b[0;36mfixed_litellm_completions\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m first_error\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py:443\u001b[0m, in \u001b[0;36mfixed_litellm_completions\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# If the completion is successful, exit the function\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/utils.py:1156\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1154\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1155\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/utils.py:1034\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/main.py:3085\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3084\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2201\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:381\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[1;32m    382\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    383\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    384\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    385\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    386\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m    387\u001b[0m     )\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: GroqException - Missing API Key - A call is being made to LLM Provider but no key is set either in the environment variables ({LLM_PROVIDER}_API_KEY) or via params",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m我的屏幕上显示什么\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/core.py:191\u001b[0m, in \u001b[0;36mOpenInterpreter.chat\u001b[0;34m(self, message, display, stream, blocking)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming_chat(message\u001b[38;5;241m=\u001b[39mmessage, display\u001b[38;5;241m=\u001b[39mdisplay)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# If stream=False, *pull* from the stream.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_streaming_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Return new messages\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/core.py:223\u001b[0m, in \u001b[0;36mOpenInterpreter._streaming_chat\u001b[0;34m(self, message, display)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_streaming_chat\u001b[39m(\u001b[38;5;28mself\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# Sometimes a little more code -> a much better experience!\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# Display mode actually runs interpreter.chat(display=False, stream=True) from within the terminal_interface.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# wraps the vanilla .chat(display=False) generator in a display.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Quite different from the plain generator stuff. So redirect to that\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display:\n\u001b[0;32m--> 223\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m terminal_interface(\u001b[38;5;28mself\u001b[39m, message)\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# One-off message\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/terminal_interface/terminal_interface.py:162\u001b[0m, in \u001b[0;36mterminal_interface\u001b[0;34m(interpreter, message)\u001b[0m\n\u001b[1;32m    154\u001b[0m             message \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    155\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    157\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    158\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_path,\n\u001b[1;32m    159\u001b[0m             }\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Is this for thine eyes?\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/core.py:259\u001b[0m, in \u001b[0;36mOpenInterpreter._streaming_chat\u001b[0;34m(self, message, display)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_messages_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# DISABLED because I think we should just not transmit images to non-multimodal models?\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# REENABLE this when multimodal becomes more common:\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# This is where it all happens!\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_respond_and_store()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Save conversation if we've turned conversation_history on\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# If it's the first message, set the conversation name\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/core.py:318\u001b[0m, in \u001b[0;36mOpenInterpreter._respond_and_store\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m last_flag_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrespond\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# For async usage\u001b[39;49;00m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_event\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpen Interpreter stopping.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/interpreter/core/respond.py:113\u001b[0m, in \u001b[0;36mrespond\u001b[0;34m(interpreter)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m     interpreter\u001b[38;5;241m.\u001b[39moffline \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message\n\u001b[1;32m    111\u001b[0m ):\n\u001b[1;32m    112\u001b[0m     output \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThere might be an issue with your API key(s).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo reset your API key (we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        Mac/Linux: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport OPENAI_API_KEY=your-key-here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Update your ~/.zshrc on MacOS or ~/.bashrc on Linux with the new key if it has already been persisted there.,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        Windows: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetx OPENAI_API_KEY your-key-here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m then restart terminal.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mtype\u001b[39m(e) \u001b[38;5;241m==\u001b[39m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRateLimitError\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsufficient_quota\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    120\u001b[0m ):\n\u001b[1;32m    121\u001b[0m     display_markdown_message(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m > You ran out of current quota for OpenAI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms API, please check your plan and billing details. You can either wait for the quota to reset or upgrade your plan.\u001b[39m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n",
      "\u001b[0;31mException\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 1582, in completion\n    response = groq_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/groq/chat/handler.py\", line 55, in completion\n    return super().completion(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/openai_like/chat/handler.py\", line 248, in completion\n    api_base, headers = self._validate_environment(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/openai_like/common_utils.py\", line 30, in _validate_environment\n    raise OpenAILikeError(\nlitellm.llms.openai_like.common_utils.OpenAILikeError: Missing API Key - A call is being made to LLM Provider but no key is set either in the environment variables ({LLM_PROVIDER}_API_KEY) or via params\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/respond.py\", line 87, in respond\n    for chunk in interpreter.llm.run(messages_for_llm):\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 324, in run\n    yield from run_text_llm(self, params)\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/llm/run_text_llm.py\", line 20, in run_text_llm\n    for chunk in llm.completions(**params):\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 466, in fixed_litellm_completions\n    raise first_error  # If all attempts fail, raise the first error\n    ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/interpreter/core/llm/llm.py\", line 443, in fixed_litellm_completions\n    yield from litellm.completion(**params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 1156, in wrapper\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 1034, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 3085, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2201, in exception_type\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 381, in exception_type\n    raise BadRequestError(\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: GroqException - Missing API Key - A call is being made to LLM Provider but no key is set either in the environment variables ({LLM_PROVIDER}_API_KEY) or via params\n\n\nThere might be an issue with your API key(s).\n\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here'. Update your ~/.zshrc on MacOS or ~/.bashrc on Linux with the new key if it has already been persisted there.,\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\n\n"
     ]
    }
   ],
   "source": [
    "interpreter.chat(\"我的屏幕上显示什么\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
